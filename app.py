"""
Token Explorer for Educators - Streamlit Application
Performance + Accessibility build:
- Cached Kaleido/Chromium configuration for Plotly image export
- Heavy renders (PNG, PDF, Word Cloud) gated behind buttons
- Cached asset generation keyed by prompt+params hash
- High-contrast theme toggle with dynamic CSS
- Global font-size control via CSS on <html>
- ARIA roles and labels on custom HTML (glossary items, probability badges)
- Human vs AI Visualization (grouped bar)
- Confidence Tracking: Continue One Token loop

Run: streamlit run app.py
"""

import os
import random
import math
from datetime import datetime
from collections import Counter

import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from huggingface_hub import InferenceClient
from huggingface_hub.errors import HfHubHTTPError
from typing import Optional

DEFAULT_HF_INFERENCE_BASE_URL = "https://router.huggingface.co/hf-inference"

# ---------------------------------------------------------------------
# Lightweight helpers
# ---------------------------------------------------------------------
def build_activity_handout_text(activity_title: str, activity: dict) -> str:
    lines = [
        f"{activity_title}",
        "=" * len(activity_title),
        "",
        f"Grade Level: {activity.get('grade_level', 'N/A')}",
        f"Duration: {activity.get('duration', 'N/A')}",
        "",
        "Description:",
        activity.get("description", ""),
        "",
        "Steps:",
    ]
    for idx, step in enumerate(activity.get("steps", []), start=1):
        lines.append(f"{idx}. {step}")
    lines.append("")
    lines.append("Learning Goals:")
    for goal in activity.get("learning_goals", []):
        lines.append(f"- {goal}")
    lines.append("")
    lines.append("Generated by Token Explorer for Educators")
    lines.append(f"Timestamp: {datetime.now().isoformat(timespec='seconds')}")
    return "\n".join(lines)

def predictions_dataframe(predictions: dict) -> pd.DataFrame:
    return pd.DataFrame(
        [
            {"Rank": idx, "Token": tok, "Probability (%)": round(prob * 100, 2)}
            for idx, (tok, prob) in enumerate(predictions.items(), start=1)
        ]
    )

# ---------------------------------------------------------------------
# Page config and base CSS
# ---------------------------------------------------------------------
st.set_page_config(
    page_title="Token Explorer for Educators",
    page_icon="üéì",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Base CSS; dynamic overrides are injected later
st.markdown("""
<style>
    .stButton>button { min-height: 44px; min-width: 44px; font-size: 16px; }
    .probability-high { background-color: #28A745; color: #FFFFFF; padding: 8px; border-radius: 6px; }
    .probability-medium { background-color: #17A2B8; color: #FFFFFF; padding: 8px; border-radius: 6px; }
    .probability-low { background-color: #FFC107; color: #111111; padding: 8px; border-radius: 6px; }
    .probability-verylow { background-color: #6C757D; color: #FFFFFF; padding: 8px; border-radius: 6px; }
    .visually-hidden { position:absolute !important; height:1px; width:1px; overflow:hidden; clip:rect(1px,1px,1px,1px); white-space:nowrap; }
</style>
""", unsafe_allow_html=True)

# ---------------------------------------------------------------------
# Content
# ---------------------------------------------------------------------
GLOSSARY = {
    "Token": {"simple": "A piece of text the model sees: a word, part of a word, or punctuation.",
              "detailed": "Tokens are the basic units. Some words split into subwords for efficiency."},
    "Probability": {"simple": "How likely the model thinks a token should come next.",
                    "detailed": "A normalized score across all possible next tokens."},
    "Temperature": {"simple": "Controls creativity vs predictability.",
                    "detailed": "Scales logits before softmax. Low = deterministic, high = diverse."},
    "Top-k": {"simple": "Keep only the k most likely tokens.",
              "detailed": "Fixed-size shortlist to reduce noise."},
    "Top-p (Nucleus)": {"simple": "Keep smallest set of tokens summing to p.",
                        "detailed": "Adaptive shortlist based on uncertainty."},
    "Perplexity": {"simple": "Lower is better. Measures confusion.",
                   "detailed": "Equivalent number of equally likely options."},
    "Entropy": {"simple": "Higher means more uncertainty.",
                "detailed": "Shannon entropy of the next-token distribution."}
}

EXAMPLE_PROMPTS = {
    "Famous Quotes": [
        "To be or not to be,",
        "I have a dream that one day",
        "Ask not what your country can do for you",
        "In the beginning was the",
        "Four score and seven years ago"
    ],
    "Story Starters": [
        "Once upon a time in a",
        "It was a dark and stormy",
        "The detective walked into the room and",
        "Long ago in a galaxy far",
        "The treasure map showed"
    ],
    "Science Facts": [
        "Water boils at",
        "The Earth revolves around",
        "Photosynthesis is the process by which",
        "DNA stands for",
        "The speed of light is"
    ],
    "Simple Sentences": [
        "The cat sat on the",
        "She walked to the",
        "Today is a beautiful",
        "The student studied for the",
        "My favorite color is"
    ],
    "Math & Logic": [
        "Two plus two equals",
        "If it rains then",
        "The square root of 16 is",
        "All mammals have",
        "When water freezes it becomes"
    ]
}

MODELS = {
    "Kimi K2 Thinking": {
        "vocab_size": 160_000,
        "context_window": 256_000,
        "languages": ["English", "Chinese"],
        "description": "Moonshot AI's reasoning-focused K2 Thinking model (INT4, 256K context).",
        "best_for": "Deep chain-of-thought, tool-augmented reasoning, long-horizon workflows",
        "hf_model_id": "moonshotai/Kimi-K2-Thinking",
        "hf_provider": "hf-inference",
    }
}

ACTIVITIES = {
    "Predict the Next Word Game": {
        "grade_level": "3-8",
        "duration": "15-20 minutes",
        "description": "Students guess the next word, compare to AI predictions.",
        "steps": [
            "Display a sentence with the last word hidden",
            "Have students write their predictions",
            "Reveal AI's top predictions with probabilities",
            "Discuss why certain words are more likely",
            "Change temperature and compare shifts"
        ],
        "learning_goals": [
            "Probability and prediction",
            "Pattern recognition",
            "Intro to AI decision-making"
        ]
    },
    "Temperature Experiment": {
        "grade_level": "6-12",
        "duration": "25-30 minutes",
        "description": "Explore how temperature affects creativity and coherence.",
        "steps": [
            "Start with the same prompt for all students",
            "Generate predictions at a low temperature (e.g., 0.2)",
            "Generate predictions at a high temperature (e.g., 1.5)",
            "Compare and discuss differences",
            "Students chart variety vs. coherence"
        ],
        "learning_goals": [
            "Understanding parameters in AI systems",
            "Balancing creativity and accuracy",
            "Data analysis and comparison"
        ]
    }
}

SIMULATED_CONTEXT_PREDICTIONS = {
    "The cat sat on the": {"mat": 0.35, "chair": 0.20, "floor": 0.15, "table": 0.12,
                           "sofa": 0.08, "bed": 0.05, "couch": 0.03, "roof": 0.02},
    "Once upon a time in a": {"kingdom": 0.40, "land": 0.25, "forest": 0.15, "village": 0.10,
                              "castle": 0.05, "city": 0.03, "galaxy": 0.02},
    "Water boils at": {"100": 0.70, "212": 0.15, "boiling": 0.05, "high": 0.04,
                       "sea": 0.03, "room": 0.02, "atmospheric": 0.01},
    "To be or not to be,": {"that": 0.90, "this": 0.03, "whether": 0.02, "it": 0.02,
                            "what": 0.01, "which": 0.01, "the": 0.01},
    "The Earth revolves around": {"the": 0.85, "Sun": 0.08, "its": 0.03, "a": 0.02,
                                  "our": 0.01, "itself": 0.01}
}

DEFAULT_SIMULATED_TOKENS = ["the", "a", "and", "is", "to", "of", "in", "it", "for", "on"]

# ---------------------------------------------------------------------
# Probability and metrics
# ---------------------------------------------------------------------
def _generate_simulated_probabilities(prompt, model_name, temperature, top_k, top_p):
    base = None
    for k in SIMULATED_CONTEXT_PREDICTIONS:
        if k.lower() in prompt.lower():
            base = SIMULATED_CONTEXT_PREDICTIONS[k].copy()
            break
    if base is None:
        base = {t: random.uniform(0.05, 0.25) for t in DEFAULT_SIMULATED_TOKENS}
        s = sum(base.values())
        base = {k: v/s for k, v in base.items()}

    # temperature scaling
    if temperature > 0:
        logits = {k: math.log(v) / max(temperature, 1e-6) for k, v in base.items()}
        m = max(logits.values())
        exps = {k: math.exp(v - m) for k, v in logits.items()}
        s = sum(exps.values())
        base = {k: v/s for k, v in exps.items()}

    # top-k
    if top_k > 0:
        items = sorted(base.items(), key=lambda x: x[1], reverse=True)[:top_k]
        base = dict(items)
        s = sum(base.values())
        base = {k: v/s for k, v in base.items()}

    # top-p
    if top_p < 1.0:
        items = sorted(base.items(), key=lambda x: x[1], reverse=True)
        csum = 0.0
        nucleus = []
        for tok, p in items:
            nucleus.append((tok, p))
            csum += p
            if csum >= top_p:
                break
        base = dict(nucleus)
        s = sum(base.values())
        base = {k: v/s for k, v in base.items()}

    return dict(sorted(base.items(), key=lambda x: x[1], reverse=True))

def _resolve_hf_token() -> Optional[str]:
    token_keys = [
        "HF_API_TOKEN",
        "HUGGINGFACEHUB_API_TOKEN",
        "HF_TOKEN",
        "HUGGING_FACE_HUB_TOKEN",
        "HF_APIKEY",
    ]

    for key in token_keys:
        token = os.getenv(key)
        if token:
            return token.strip()

    secrets_obj = getattr(st, "secrets", None)
    if secrets_obj:
        for key in token_keys + [k.lower() for k in token_keys]:
            try:
                token = secrets_obj.get(key) if hasattr(secrets_obj, "get") else secrets_obj[key]
            except KeyError:
                token = None
            if token:
                return str(token).strip()

    return None


def _resolve_hf_endpoint() -> Optional[str]:
    endpoint_keys = [
        "HF_INFERENCE_ENDPOINT",
        "HUGGINGFACEHUB_INFERENCE_ENDPOINT",
        "HUGGINGFACEHUB_ENDPOINT",
        "HF_ENDPOINT",
    ]

    for key in endpoint_keys:
        endpoint = os.getenv(key)
        if endpoint:
            return endpoint.strip()

    secrets_obj = getattr(st, "secrets", None)
    if secrets_obj:
        for key in endpoint_keys + [k.lower() for k in endpoint_keys]:
            try:
                endpoint = secrets_obj.get(key) if hasattr(secrets_obj, "get") else secrets_obj[key]
            except KeyError:
                endpoint = None
            if endpoint:
                return str(endpoint).strip()

    return None


def _resolve_hf_base_url() -> Optional[str]:
    base_keys = [
        "HF_INFERENCE_BASE_URL",
        "HUGGINGFACEHUB_INFERENCE_BASE_URL",
        "HF_BASE_URL",
        "HUGGING_FACE_BASE_URL",
    ]

    for key in base_keys:
        base_url = os.getenv(key)
        if base_url:
            return base_url.strip()

    secrets_obj = getattr(st, "secrets", None)
    if secrets_obj:
        for key in base_keys + [k.lower() for k in base_keys]:
            try:
                base_url = secrets_obj.get(key) if hasattr(secrets_obj, "get") else secrets_obj[key]
            except KeyError:
                base_url = None
            if base_url:
                return str(base_url).strip()

    return None

@st.cache_resource(show_spinner=False)
def _get_hf_client(
    model_id: Optional[str],
    token: Optional[str],
    endpoint: Optional[str],
    provider_override: Optional[str] = None
) -> InferenceClient:
    if not token:
        raise ValueError("An authentication token is required for Hugging Face Inference.")

    client_kwargs = {"token": token}

    if endpoint:
        client_kwargs["model"] = endpoint.rstrip("/")
    elif model_id:
        client_kwargs["model"] = model_id
        if provider_override:
            provider = provider_override.strip()
        else:
            provider = os.getenv("HF_INFERENCE_PROVIDER", "hf-inference").strip()
        if provider:
            client_kwargs["provider"] = provider
        base_url = _resolve_hf_base_url() or DEFAULT_HF_INFERENCE_BASE_URL
        client_kwargs["base_url"] = base_url.rstrip("/")
    else:
        raise ValueError("A Hugging Face model id or inference endpoint must be provided.")

    return InferenceClient(**client_kwargs)

def _maybe_notify_once(key: str, message: str, level: str = "warning"):
    flag_key = f"__notification_shown_{key}"
    if not st.session_state.get(flag_key, False):
        notifier = getattr(st, level, st.warning)
        notifier(message)
        st.session_state[flag_key] = True

def generate_probabilities(prompt, model_name, temperature, top_k, top_p):
    model_meta = MODELS.get(model_name, {})
    hf_model_id = model_meta.get("hf_model_id")
    hf_provider = model_meta.get("hf_provider")

    if not hf_model_id:
        _maybe_notify_once(
            f"hf_missing_model_{model_name}",
            "Real Hugging Face predictions are not yet available for this model profile. Falling back to classroom simulator.",
            level="info"
        )
        return _generate_simulated_probabilities(prompt, model_name, temperature, top_k, top_p)

    hf_token = _resolve_hf_token()
    hf_endpoint = _resolve_hf_endpoint()
    if not hf_token:
        _maybe_notify_once(
            "hf_missing_token",
            "Set the HF_API_TOKEN secret or environment variable to enable live probabilities from Hugging Face. Using simulator data instead.",
            level="warning"
        )
        return _generate_simulated_probabilities(prompt, model_name, temperature, top_k, top_p)

    try:
        client = _get_hf_client(hf_model_id, hf_token, hf_endpoint, provider_override=hf_provider)

        request_kwargs = {
            "max_new_tokens": 1,
            "return_full_text": False,
            "details": True,
            "stream": False,
        }

        if temperature <= 0.0:
            request_kwargs["do_sample"] = False
        else:
            request_kwargs["do_sample"] = True
            request_kwargs["temperature"] = float(temperature)
            request_kwargs["top_p"] = float(top_p)
            if top_k > 0:
                request_kwargs["top_k"] = int(top_k)

        if top_k > 0 and "top_k" not in request_kwargs:
            request_kwargs["top_k"] = int(top_k)

        response = client.text_generation(prompt, **request_kwargs)
    except StopIteration:
        _maybe_notify_once(
            f"hf_provider_unavailable_{model_name}",
            "Hugging Face Inference response stream ended unexpectedly. Verify the Hugging Face configuration or retry shortly; using simulator data for now.",
            level="warning"
        )
        return _generate_simulated_probabilities(prompt, model_name, temperature, top_k, top_p)
    except HfHubHTTPError as exc:
        status = getattr(getattr(exc, "response", None), "status_code", None)
        if status == 401:
            msg = "Hugging Face API rejected the token (401 Unauthorized). Verify `HF_API_TOKEN` has Inference API access."
        elif status == 404:
            msg = f"Hugging Face model `{hf_model_id}` not found or not yet available for the selected provider. Double-check the model id or set `HF_INFERENCE_PROVIDER` accordingly. Falling back to simulator."
        elif status == 410:
            msg = (
                "Hugging Face has migrated Inference to `https://router.huggingface.co/hf-inference`. "
                "Set `HF_INFERENCE_BASE_URL` to the new router or update dependencies. Using simulator data."
            )
        else:
            msg = f"Hugging Face request failed ({status or 'HTTP error'}). Reverting to simulator data."
        _maybe_notify_once(
            f"hf_http_error_{model_name}",
            msg,
            level="warning"
        )
        return _generate_simulated_probabilities(prompt, model_name, temperature, top_k, top_p)
    except Exception as exc:
        _maybe_notify_once(
            f"hf_error_{model_name}",
            f"Hugging Face request failed ({type(exc).__name__}: {exc}). Reverting to simulator data.",
            level="error"
        )
        return _generate_simulated_probabilities(prompt, model_name, temperature, top_k, top_p)

    details = getattr(response, "details", None)
    tokens = getattr(details, "tokens", None) if details else None

    if not tokens:
        _maybe_notify_once(
            f"hf_no_details_{model_name}",
            "Hugging Face response did not include probability details. Using simulator data.",
            level="warning"
        )
        return _generate_simulated_probabilities(prompt, model_name, temperature, top_k, top_p)

    first_token = next((tok for tok in tokens if not getattr(tok, "special", False)), None)
    if first_token is None:
        return _generate_simulated_probabilities(prompt, model_name, temperature, top_k, top_p)

    candidate_predictions = {}

    top_candidates = getattr(first_token, "top_tokens", None)
    if top_candidates:
        for candidate in top_candidates:
            text = getattr(candidate, "text", None)
            logprob = getattr(candidate, "logprob", None)
            if text is None or logprob is None:
                continue
            clean_text = text if text.strip() else text
            prob = math.exp(logprob)
            if clean_text in candidate_predictions:
                candidate_predictions[clean_text] += prob
            else:
                candidate_predictions[clean_text] = prob
    else:
        token_text = getattr(first_token, "text", "") or ""
        logprob = getattr(first_token, "logprob", None)
        if token_text and logprob is not None:
            clean_text = token_text if token_text.strip() else token_text
            candidate_predictions[clean_text] = math.exp(logprob)

    total_prob = sum(candidate_predictions.values())
    if not candidate_predictions or total_prob <= 0:
        return _generate_simulated_probabilities(prompt, model_name, temperature, top_k, top_p)

    normalized = {tok: val / total_prob for tok, val in candidate_predictions.items() if tok}
    sorted_preds = dict(sorted(normalized.items(), key=lambda x: x[1], reverse=True))

    return sorted_preds

def calculate_entropy(probabilities):
    e = 0.0
    for p in probabilities.values():
        if p > 0:
            e -= p * math.log2(p)
    return e

def calculate_perplexity(entropy):
    return 2 ** entropy

# ---------------------------------------------------------------------
# Charts
# ---------------------------------------------------------------------
def create_probability_chart(predictions):
    tokens = list(predictions.keys())[:10]
    probs = [predictions[t] * 100 for t in tokens]
    colors_list = []
    for p in probs:
        if p > 50: colors_list.append('#28A745')
        elif p > 20: colors_list.append('#17A2B8')
        elif p > 5: colors_list.append('#FFC107')
        else: colors_list.append('#6C757D')
    fig = go.Figure(data=[go.Bar(
        y=tokens, x=probs, orientation='h',
        marker=dict(color=colors_list),
        text=[f'{p:.1f}%' for p in probs],
        textposition='outside'
    )])
    fig.update_layout(
        title="Top 10 Token Probabilities",
        xaxis_title="Probability (%)",
        yaxis_title="Token",
        height=500,
        showlegend=False,
        yaxis={'categoryorder': 'total ascending'}
    )
    return fig

def create_entropy_chart(entropy_values):
    if not entropy_values:
        fig = go.Figure()
        fig.update_layout(title="Entropy Over Token Sequence",
                          xaxis_title="Step", yaxis_title="Entropy (bits)")
        return fig
    fig = go.Figure(data=[go.Scatter(
        x=list(range(1, len(entropy_values)+1)),
        y=entropy_values,
        mode='lines+markers',
        marker=dict(size=8, color='#0066CC'),
        line=dict(width=2, color='#0066CC')
    )])
    fig.update_layout(
        title="Entropy Over Token Sequence",
        xaxis_title="Step",
        yaxis_title="Entropy (bits)",
        height=400
    )
    return fig

@st.cache_data(show_spinner=False)
def build_prediction_report_text(prompt_text: str, params: dict, metrics: dict, predictions: dict) -> str:
    lines = [
        "Token Explorer for Educators ‚Äî Prediction Snapshot",
        f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "",
        "Prompt:",
        prompt_text or "(none)",
        "",
        "Parameters:",
        f"- Temperature: {params.get('temperature')}",
        f"- Top-k: {params.get('top_k')}",
        f"- Top-p: {params.get('top_p')}",
        f"- Model: {params.get('model_name')}",
        "",
        "Metrics:",
        f"- Entropy: {metrics.get('entropy'):.2f} bits" if metrics.get("entropy") is not None else "- Entropy: n/a",
        f"- Perplexity: {metrics.get('perplexity'):.2f}" if metrics.get('perplexity') is not None else "- Perplexity: n/a",
        f"- Top Token Probability: {max(predictions.values())*100:.1f}%" if predictions else "- Top Token Probability: n/a",
        "",
        "Top Tokens:",
    ]
    for idx, (tok, prob) in enumerate(predictions.items(), start=1):
        lines.append(f"{idx}. {tok} ‚Äî {prob*100:.2f}%")
    lines.append("")
    lines.append("Tip: Copy this summary into your lesson plan or share it with students.")
    return "\n".join(lines)


# ---------------------------------------------------------------------
# Session state defaults
# ---------------------------------------------------------------------
def _ensure_state_defaults():
    st.session_state.setdefault('tutorial_shown', False)
    st.session_state.setdefault('show_tutorial', False)
    st.session_state.setdefault('show_glossary', False)
    st.session_state.setdefault('poll_mode', False)
    st.session_state.setdefault('student_predictions', [])

    # Accessibility
    st.session_state.setdefault('high_contrast', False)
    st.session_state.setdefault('font_size', 'Medium')  # Small / Medium / Large

    # Params
    st.session_state.setdefault('temperature', 1.0)
    st.session_state.setdefault('top_k', 50)
    st.session_state.setdefault('top_p', 0.9)

    # Current prediction context
    st.session_state.setdefault('input_text', '')
    st.session_state.setdefault('current_text', '')
    st.session_state.setdefault('current_model', list(MODELS.keys())[0])
    st.session_state.setdefault('predictions', None)
    st.session_state.setdefault('entropy', None)
    st.session_state.setdefault('perplexity', None)

    # Confidence tracking sequence
    st.session_state.setdefault('sequence_tokens', [])
    st.session_state.setdefault('sequence_entropies', [])
    st.session_state.setdefault('sequence_top1_probs', [])

_ensure_state_defaults()

# ---------------------------------------------------------------------
# Dynamic style injection
# ---------------------------------------------------------------------
def apply_dynamic_styles():
    font_map = {"Small": "14px", "Medium": "16px", "Large": "18px"}
    base_size = font_map.get(st.session_state.get("font_size", "Medium"), "16px")

    css_parts = [f"""
    <style>
      html {{ font-size: {base_size}; }}
      body, .markdown-text-container, .stMarkdown, .stText, .stRadio, .stSelectbox, .stMultiSelect,
      .stDataFrame, .stMetric, .stTextInput, .stTextArea, .stSlider, .stDownloadButton, .stButton, .stExpander,
      .stTabs, .stTab {{ font-size: 1rem; line-height: 1.5; }}
      .stTextInput input, .stTextArea textarea, .stSelectbox div[data-baseweb="select"],
      .stButton>button, .stDownloadButton>button {{ font-size: 1rem; }}
    """]

    if st.session_state.get("high_contrast", False):
        css_parts.append("""
        html, body { background-color: #000000 !important; color: #FFFFFF !important; }
        .stApp, .block-container { background-color: #000000 !important; }
        h1, h2, h3, h4, h5, h6, p, li, label, span, div { color: #FFFFFF !important; }
        a { color: #00E5FF !important; text-decoration: underline; }
        .stButton>button, .stDownloadButton>button {
            background-color: #FFFFFF !important; color: #000000 !important; border: 2px solid #FFFFFF !important;
        }
        .stTextInput>div>div>input, .stTextArea textarea, .stSelectbox div[role="combobox"] {
            background-color: #111111 !important; color: #FFFFFF !important; border: 1px solid #FFFFFF !important;
        }
        .stDataFrame, .dataframe, .stTable { filter: invert(1) hue-rotate(180deg); }
        .probability-high { background-color: #00A65A !important; color: #FFFFFF !important; }
        .probability-medium { background-color: #148EA1 !important; color: #FFFFFF !important; }
        .probability-low { background-color: #C19A00 !important; color: #111111 !important; }
        .probability-verylow { background-color: #888888 !important; color: #FFFFFF !important; }
        """)
    css_parts.append("</style>")
    st.markdown("\n".join(css_parts), unsafe_allow_html=True)

# ---------------------------------------------------------------------
# Accessible HTML helpers
# ---------------------------------------------------------------------
def render_probability_badge(rank: int, token: str, prob: float) -> str:
    pct = prob * 100
    if pct > 50: cls = "probability-high"
    elif pct > 20: cls = "probability-medium"
    elif pct > 5: cls = "probability-low"
    else: cls = "probability-verylow"
    label = f"Rank {rank}. Token {token}. Probability {pct:.1f} percent."
    return (
        f'<div class="{cls}" role="note" aria-label="{label}" aria-live="polite">'
        f'#{rank}: <strong>{token}</strong> ‚Äî {pct:.1f}%'
        f'</div>'
    )

def render_glossary_item(term: str, simple: str, detailed: str) -> str:
    term_id = f"glossary-{term.replace(' ', '-').lower()}"
    return f"""
    <section role="listitem" aria-labelledby="{term_id}">
      <h4 id="{term_id}" role="heading" aria-level="4">{term}</h4>
      <p role="note" aria-label="Simple definition for {term}">{simple}</p>
      <p role="note" aria-label="Detailed definition for {term}">{detailed}</p>
    </section>
    """

# ---------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------
def main():
    st.title("üéì Token Explorer for Educators")
    st.markdown("### Visualize token-by-token prediction, confidence, and classroom activities")

    if not st.session_state.tutorial_shown:
        with st.expander("üëã Quick Start", expanded=True):
            st.markdown("""
1) Enter text or load an example  
2) Choose a model  
3) Set temperature / top-k / top-p  
4) Generate predictions  
5) Use **Continue One Token** to step and track entropy  
6) Use **Class Poll Mode** for Human vs AI visualization  
7) Print **Activity Handouts** for the classroom  
8) Exports render on demand to keep the app fast
            """)
            if st.button("Got it"):
                st.session_state.tutorial_shown = True
                st.rerun()

    # Top controls
    top_c1, top_c2, top_c3, top_c4, top_c5 = st.columns([2,2,2,1,1])
    with top_c1:
        if st.button("üìñ Glossary"):
            st.session_state.show_glossary = not st.session_state.show_glossary
    with top_c2:
        if st.button("‚ùì Help"):
            st.session_state.show_tutorial = not st.session_state.show_tutorial
    with top_c3:
        st.session_state.poll_mode = st.checkbox("üìä Class Poll Mode", value=st.session_state.poll_mode)
    with top_c4:
        st.session_state.high_contrast = st.checkbox("üåì High Contrast", value=st.session_state.high_contrast)
    with top_c5:
        st.session_state.font_size = st.selectbox("Font", ["Small","Medium","Large"],
                                                  index=["Small","Medium","Large"].index(st.session_state.font_size),
                                                  label_visibility="collapsed")

    apply_dynamic_styles()
    st.markdown("---")

    if st.session_state.show_glossary:
        with st.expander("üìñ Glossary", expanded=True):
            st.markdown('<div role="list" aria-label="AI glossary terms">', unsafe_allow_html=True)
            for term, d in GLOSSARY.items():
                st.markdown(render_glossary_item(term, d['simple'], d['detailed']), unsafe_allow_html=True)
            st.markdown('</div>', unsafe_allow_html=True)

    col_left, col_mid, col_right = st.columns([1,2,1])

    # Left: input + models
    with col_left:
        st.markdown("### üìù Input")
        cat = st.selectbox("Load Example", ["-- Select --"] + list(EXAMPLE_PROMPTS.keys()))
        if cat != "-- Select --":
            ex = st.selectbox("Example", EXAMPLE_PROMPTS[cat])
            if st.button("Load Example"):
                st.session_state.input_text = ex
                st.session_state.current_text = ex
        if st.button("üé≤ Random Example"):
            rcat = random.choice(list(EXAMPLE_PROMPTS.keys()))
            rval = random.choice(EXAMPLE_PROMPTS[rcat])
            st.session_state.input_text = rval
            st.session_state.current_text = rval

        st.session_state.input_text = st.text_area(
            "Enter text", value=st.session_state.get('input_text', ''), height=150
        )
        if st.session_state.input_text != st.session_state.current_text:
            st.session_state.current_text = st.session_state.input_text

        st.markdown("### ü§ñ Model")
        available_models = list(MODELS.keys())
        model_name = st.selectbox(
            "Choose Model",
            available_models,
            index=available_models.index(st.session_state.get('current_model'))
        )
        st.session_state.current_model = model_name
        mi = MODELS[model_name]
        context_line = (
            f"\nüß† Context: {mi['context_window']:,} tokens"
            if mi.get("context_window")
            else ""
        )
        st.info(
            f"**{model_name}**  \n"
            f"üìä Vocab: {mi['vocab_size']:,}  \n"
            f"üåç Languages: {', '.join(mi['languages'][:3])}{'...' if len(mi['languages'])>3 else ''}  \n"
            f"‚ú® Best for: {mi['best_for']}"
            f"{context_line}"
        )

        compare_models = False
        model_name_2 = None
        if len(available_models) > 1:
            compare_models = st.checkbox("üîÑ Compare with another model")
            if compare_models:
                compare_options = [m for m in available_models if m != model_name]
                if compare_options:
                    model_name_2 = st.selectbox("Second Model", compare_options)

    # Middle: Parameters inside a form to reduce reruns
    with col_mid:
        st.markdown("### üéöÔ∏è Parameters")
        with st.form("params_form", clear_on_submit=False):
            p1, p2, p3 = st.columns(3)
            with p1:
                conservative = st.form_submit_button("üõ°Ô∏è Conservative")
            with p2:
                balanced = st.form_submit_button("‚öñÔ∏è Balanced")
            with p3:
                creative = st.form_submit_button("üé® Creative")

            temp = st.slider("üå°Ô∏è Temperature", 0.0, 2.0, st.session_state.get('temperature', 1.0), 0.1)
            top_k = st.slider("üîù Top-k", 0, 100, st.session_state.get('top_k', 50), 5)
            top_p = st.slider("üéØ Top-p", 0.0, 1.0, st.session_state.get('top_p', 0.9), 0.05)
            apply_params = st.form_submit_button("Apply")

        if conservative:
            st.session_state.update(temperature=0.3, top_k=10, top_p=0.8)
        elif balanced:
            st.session_state.update(temperature=0.8, top_k=50, top_p=0.9)
        elif creative:
            st.session_state.update(temperature=1.5, top_k=100, top_p=0.95)
        elif apply_params:
            st.session_state.update(temperature=temp, top_k=top_k, top_p=top_p)

        temperature = st.session_state.get('temperature', 1.0)
        top_k = st.session_state.get('top_k', 50)
        top_p = st.session_state.get('top_p', 0.9)

        if temperature == 0: strategy = "üîí Greedy"
        elif top_k > 0 and top_p < 1.0: strategy = f"üéØ Top-k ({top_k}) + Top-p ({top_p})"
        elif top_k > 0: strategy = f"üîù Top-k ({top_k})"
        elif top_p < 1.0: strategy = f"üéØ Nucleus (Top-p={top_p})"
        else: strategy = "üå°Ô∏è Temperature sampling"
        st.info(f"**Decoding Strategy:** {strategy}")

        gen_c1, gen_c2 = st.columns([2,1])
        with gen_c1:
            if st.button("üöÄ Generate Predictions", type="primary", use_container_width=True):
                text = st.session_state.current_text.strip()
                if text:
                    preds = generate_probabilities(text, model_name, temperature, top_k, top_p)
                    st.session_state.predictions = preds
                    ent = calculate_entropy(preds); ppl = calculate_perplexity(ent)
                    st.session_state.entropy = ent; st.session_state.perplexity = ppl
                    # reset confidence tracking
                    st.session_state.sequence_tokens = []
                    st.session_state.sequence_entropies = []
                    st.session_state.sequence_top1_probs = []
                    if compare_models and model_name_2:
                        st.session_state.predictions_2 = generate_probabilities(text, model_name_2, temperature, top_k, top_p)
                else:
                    st.warning("Enter some text.")
        with gen_c2:
            if st.button("‚û°Ô∏è Continue One Token", use_container_width=True):
                if st.session_state.get('predictions'):
                    curr_preds = st.session_state.predictions
                    curr_entropy = calculate_entropy(curr_preds)
                    top_token, top_prob = next(iter(curr_preds.items()))
                    st.session_state.sequence_entropies.append(curr_entropy)
                    st.session_state.sequence_top1_probs.append(top_prob)
                    st.session_state.sequence_tokens.append(top_token)

                    new_text = (st.session_state.current_text + " " + top_token).strip()
                    st.session_state.current_text = new_text
                    st.session_state.input_text = new_text

                    new_preds = generate_probabilities(new_text, st.session_state.current_model, temperature, top_k, top_p)
                    st.session_state.predictions = new_preds
                    ent = calculate_entropy(new_preds); ppl = calculate_perplexity(ent)
                    st.session_state.entropy = ent; st.session_state.perplexity = ppl
                else:
                    st.info("Generate predictions first.")

        if st.session_state.get('predictions'):
            st.markdown("---")
            st.markdown("### üéØ Predictions", help="Top tokens with probabilities and uncertainty metrics.")
            predictions = st.session_state.predictions

            max_prob = max(predictions.values())
            if max_prob > 0.5: st.success("‚úÖ High confidence (>50%)")
            elif max_prob > 0.2: st.info("‚ÑπÔ∏è Medium confidence (20‚Äì50%)")
            else: st.warning("‚ö†Ô∏è Low confidence (<20%)")

            m1, m2 = st.columns(2)
            with m1: st.metric("üìä Entropy", f"{st.session_state.entropy:.2f} bits")
            with m2: st.metric("üé≤ Perplexity", f"{st.session_state.perplexity:.1f}")

            st.markdown('<div role="list" aria-label="Top tokens by probability" aria-live="polite">', unsafe_allow_html=True)
            for i, (tok, p) in enumerate(list(predictions.items())[:10], 1):
                st.markdown(render_probability_badge(i, tok, p), unsafe_allow_html=True)
            st.markdown('</div>', unsafe_allow_html=True)

            st.markdown("---")
            st.markdown("### üìä Visualizations")
            tab1, tab2, tab3 = st.tabs([
                "üìä Probability Chart", "üìã Token Table", "üìâ Confidence Tracking"
            ])

            with tab1:
                fig = create_probability_chart(predictions)
                st.plotly_chart(fig, use_container_width=True)

            with tab2:
                df_probs = predictions_dataframe(predictions)
                st.dataframe(df_probs.set_index("Rank"), use_container_width=True)

                csv_data = df_probs.to_csv(index=False).encode("utf-8")
                st.download_button(
                    "Download Probabilities (CSV)",
                    data=csv_data,
                    file_name=f"token_probabilities_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv",
                    use_container_width=True
                )

                summary_text = build_prediction_report_text(
                    prompt_text=st.session_state.get('current_text',''),
                    params={'temperature': temperature, 'top_k': top_k, 'top_p': top_p,
                            'model_name': st.session_state.get('current_model','')},
                    metrics={'entropy': st.session_state.get('entropy'),
                             'perplexity': st.session_state.get('perplexity')},
                    predictions=predictions
                )
                st.download_button(
                    "Download Summary (TXT)",
                    data=summary_text.encode("utf-8"),
                    file_name=f"token_explorer_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                    mime="text/plain",
                    use_container_width=True
                )

            with tab3:
                st.markdown("Each **Continue One Token** records the current distribution entropy and top-1 probability before appending.")
                seq_ent = st.session_state.sequence_entropies
                seq_p1 = st.session_state.sequence_top1_probs
                seq_tok = st.session_state.sequence_tokens

                st.plotly_chart(create_entropy_chart(seq_ent), use_container_width=True)

                if seq_tok:
                    df_steps = pd.DataFrame({
                        "Step": list(range(1, len(seq_tok)+1)),
                        "Chosen Token": seq_tok,
                        "Top-1 Probability (%)": [round(p*100, 2) for p in seq_p1],
                        "Entropy (bits)": [round(e, 3) for e in seq_ent]
                    })
                    st.dataframe(df_steps, use_container_width=True)
                else:
                    st.info("No steps yet. Click **Continue One Token** to start tracking.")

                if st.button("‚ôªÔ∏è Reset Tracking"):
                    st.session_state.sequence_tokens = []
                    st.session_state.sequence_entropies = []
                    st.session_state.sequence_top1_probs = []
                    st.success("Tracking reset.")

    # Right: activities + handout export
    with col_right:
        st.markdown("### üè´ Classroom Activities")
        act = st.selectbox("Choose Activity", ["-- Select --"] + list(ACTIVITIES.keys()))
        if act != "-- Select --":
            a = ACTIVITIES[act]
            with st.expander(f"üìã {act}", expanded=True):
                st.markdown(f"**Grade Level**: {a['grade_level']}  \n**Duration**: {a['duration']}")
                st.markdown(f"**Description**: {a['description']}")
                st.markdown("**Steps:**")
                for i, step in enumerate(a['steps'], 1):
                    st.markdown(f"{i}. {step}")
                st.markdown("**Learning Goals:**")
                for g in a['learning_goals']:
                    st.markdown(f"- {g}")

            handout_text = build_activity_handout_text(act, a)
            st.download_button(
                label="Download Activity Summary (TXT)",
                data=handout_text.encode("utf-8"),
                file_name=f"{act.replace(' ', '_').lower()}_handout_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                mime="text/plain",
                use_container_width=True
            )

    # Class Poll Mode
    if st.session_state.poll_mode:
        st.markdown("---")
        st.markdown("## üìä Class Poll Mode")
        c1, c2 = st.columns(2)

        with c1:
            st.markdown("### üë• Student Submissions")
            st.markdown(f"**Join Code**: POLL-{random.randint(1000, 9999)}")
            guess = st.text_input("Your prediction:", key="student_guess")
            if st.button("Submit Prediction"):
                if guess:
                    st.session_state.student_predictions.append(guess.strip().lower())
                    st.success("Submitted.")
            st.metric("Total Submissions", len(st.session_state.student_predictions))
            if st.button("üóëÔ∏è Clear All Predictions"):
                st.session_state.student_predictions = []
                st.rerun()

        with c2:
            st.markdown("### üß† Human vs AI")
            if st.session_state.student_predictions and st.session_state.get('predictions'):
                from collections import Counter
                counts = Counter(st.session_state.student_predictions)
                total = len(st.session_state.student_predictions)
                top5_students = counts.most_common(5)
                student_df = pd.DataFrame(
                    {"Token": [w for w, _ in top5_students],
                     "Percent": [(c/total)*100 for _, c in top5_students]}
                )

                ai_items = list(st.session_state.predictions.items())[:5]
                ai_df = pd.DataFrame(
                    {"Token": [t for t, _ in ai_items],
                     "Percent": [p*100 for _, p in ai_items]}
                )

                tokens_union = sorted(set(student_df["Token"]).union(set(ai_df["Token"])))
                combined_rows = []
                for tok in tokens_union:
                    s_val = float(student_df.loc[student_df["Token"] == tok, "Percent"].iloc[0]) if (student_df["Token"] == tok).any() else 0.0
                    a_val = float(ai_df.loc[ai_df["Token"] == tok, "Percent"].iloc[0]) if (ai_df["Token"] == tok).any() else 0.0
                    combined_rows.append({"Token": tok, "Source": "Students", "Percent": s_val})
                    combined_rows.append({"Token": tok, "Source": "AI", "Percent": a_val})

                combined_df = pd.DataFrame(combined_rows)

                st.dataframe(
                    pd.concat([
                        student_df.assign(Source="Students"),
                        ai_df.assign(Source="AI")
                    ], ignore_index=True),
                    use_container_width=True
                )

                fig_cmp = px.bar(
                    combined_df,
                    x="Token", y="Percent",
                    color="Source",
                    barmode="group",
                    title="Student vs AI Predictions",
                    text=combined_df["Percent"].round(1).astype(str) + "%"
                )
                fig_cmp.update_layout(yaxis_title="Percent (%)", xaxis_title="Token", height=450)
                st.plotly_chart(fig_cmp, use_container_width=True)

                top_student = top5_students[0][0]
                top_ai = ai_items[0][0]
                if top_student == top_ai:
                    st.success(f"‚úÖ Agreement on '{top_student}'")
                else:
                    st.info(f"Students: '{top_student}' vs AI: '{top_ai}'")
            else:
                st.info("Need at least one student submission and an AI prediction to display the comparison.")

    # Footer
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: #6C757D; padding: 20px;'>
      <p><strong>Token Explorer for Educators</strong> | Fast & Accessible Edition</p>
      <p>On-demand exports and caching reduce load time</p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
